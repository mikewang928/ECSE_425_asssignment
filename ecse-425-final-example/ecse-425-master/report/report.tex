\documentclass[12pt]{IEEEtran} % 2014 rules say 12pt font
\usepackage[top=0.7in, bottom=0.7in, left=0.7in, right=0.7in]{geometry}  

\usepackage[bookmarks, colorlinks, breaklinks, 
    pdftitle={Group 10 Report},
    pdfauthor={David Lavoie-Boutin},
    pdfsubject={VHDL implementation of MIPS processor},
    pdfcreator={Sublime Text with LaTeXtools},
    pdfdisplaydoctitle={true}]
    {hyperref}
\usepackage{pdflscape} %landscape
\usepackage{epstopdf} % for including .eps graphics
\usepackage{listings} % insert code
\usepackage{pdfpages} % include other pdf
\usepackage{graphicx} % include images

\usepackage[section]{placeins}

\lstset{
  language=[x86masm]Assembler,
  basicstyle=\small,
  breaklines=true
  }

\title{VHDL implementation of a pipelined MIPS processor}

\date{\today}
\author{Wei-Di Chang, David Lavoie-Boutin, Muhammad Ali Lashari}

\begin{document}
\twocolumn[
\begin{@twocolumnfalse}
\maketitle
\end{@twocolumnfalse}
\begin{abstract}
    The MIPS processor is the most widely used processor example in academia for it is simple enough to understand and teach with its classic RISC architecture yet can implement new architectural design with little changes to the core architecture. In this article, we will present our attempt at implementing a MIPS processor in VHDL along with some optimization we tried. The different optimization are also compared to find an optimal design amongst those tried.
\end{abstract}
\bigskip]

\section{Introduction} % (fold)
\label{sec:introduction}

In the course of a month, we have implemented a VHDL description of a MIPS processor and simulated its behaviour in ModelSim. This report will explain the design of the processor, and describe our attempt at optimising its performance during branching instructions. 
% section introduction (end)

\section{Design} % (fold)
\label{sec:design}

\subsection{Fetch Stage} % (fold)
\label{sub:fetch_stage}
The fetch stage wraps two main components with some glue logic. The first is the instruction memory, which is taken from the third project deliverable. The testing script loads an assembled binary program in the memory structure. The address fetched in memory is driven be the PC component which is a basic register holding the value of the program counter. This register is controlled with an enable and reset line. The enable line simply ensures the input of PC is passed to the output. In case it is low, the output is taken from the last recorded input.

These two components are connected with some logic to allow program counter jumps in the case of jump or branch instructions. Specifically, a multiplexer allows the selection of either the incremented program counter or the address computed in decode as the source for the next pc value.  
% subsection fetch_stage (end)

\subsection{Decode Stage} % (fold)
\label{sub:decode_stage}
The decode stage is responsible for extracting the information needed to execute the instruction. It parses which instruction type we received, either I-Type, R-Type or Branch-Type. Based on this information, it extracts which register should be used as operands for the execute stage and which is the destination register. Additionally, it generates control signals driving the multiplexers in the execute stage that select between register content and immediate value for the second input to the ALU. 

The decode stage also implements early branch address resolution. This is done by adding dedicated hardware to compute the \verb@base + offset@ at every cycle. Additional logic is added exclusively to analyse if the current is a \verb@BEQ@, \verb@BNE@ or jump instruction. If this is the case, that logic drives the branch selection multiplexer in the fetch to use the output of the address computation hardware as next program address.

% subsection decode_stage (end)

\subsection{Execute Stage} % (fold)
\label{sub:execute_stage}
The execute stage integrates 3 multiplexers and one Arithmetic Logic Unit, ALU for short. One multiplexer selects the source of the second ALU operand based on a control signal from the decode stage. Two mutiplexers are then able to choose from the data output by the decode and data forwarded from later stages. These multiplexers are controlled by the forwarding unit and ensure the correct operands are selected for the ALU unit. \\
The use of this structure allows us to combine many operations. Specifically, the ALU does not need to differentiate between R-type and I-type instructions. This allows the ALU to operate based on a 4 bits op-code, which can differentiate up to 16 functions. 
% subsection execute_stage (end)

\subsection{Memory and Write Back} % (fold)
\label{sub:memory_and_write_back}
The memory stage has been implemented using a data memory block and 3 multiplexers.

The first multiplexer is used to select between data pass-through to the Write Back stage or data write to the data memory. The second multiplexer is used to select the input to the data memory, selecting as input either normal data lines or forwarded data lines. Finally, the last multiplexer is used to signal that the address that gets transferred to the data memory block is meant to be used (load or store).

The data memory is separate from instruction memory and has a depth d = 65536*4, indexed by bytes, resulting in a capacity of 262143 bytes, or 2097144 bits. It always has data available on its output line, enabling zero delay consecutive read, and a one cycle write delay.

The write back stage is simply a group of data lines buffered back into the decode stage, hence the write back stage contains no functional components. 

% subsection memory_and_write_back (end)

\subsection{Stage Interconnection} % (fold)
\label{sub:stage_interconnection}
Stages are connected through falling-edge triggered buffers. The inputs of each stage are latched to the outputs of the previous stage every falling edge. However, the stall triggering signal originating from decode on branches and jumps are fed into the EX stage via two sequential buffers. This is because the stall command must affect the instruction after the branch/jump instruction since it has been incorrectly fetched. These incorrectly fetched instructions are decoded properly but subsequently nullified in the EX stage.
% subsection stage_interconnection (end)

\subsection{Hazard Detection} % (fold)
\label{sub:hazard_detection}
The hazard detection logic serves to stall the pipeline if a read after write hazard is detected. For standard operations, data can be forwarded back from later stages. However a load instruction require extra logic for stall incurs.\\
\\
\begin{tabular}{l l}
    Given Instruction & Executed Instructions\\
    \hline 
    \begin{lstlisting}
Lw $3, 0($5)
Addi $3, $0, 1
    \end{lstlisting} & 
    \begin{lstlisting}
Lw $3, 0($5)
-- stall
Addi $3, $0, 1
    \end{lstlisting} \\
    \hline
    \begin{lstlisting}
Lw $3, 0($5)
Sw $3, 0($6)
    \end{lstlisting} &
    \begin{lstlisting}
Lw $3, 0($5)
Sw $3, 0($6)
    \end{lstlisting}\\
    \hline 
    \begin{lstlisting}
Lw $3, 0($5)
Sw $4, 0($3)
    \end{lstlisting} &
    \begin{lstlisting}
Lw $3, 0($5)
-- stall
Sw $4, 0($3)
    \end{lstlisting} \\
\end{tabular}
\vspace{0.3 cm}\\
As shown in the table above, a stall is only triggered if the preceding instruction is a load that writes to the same register as one or two of the EX stage operand registers of the current instruction. \\
In the case of a load instruction writing to a register that is used to write to memory in a store instruction in the next cycle, no stalling is necessary since forwarding will be used to handle that. A stall gives the processor a single cycle delay to retrieve data from memory stage so that it may be used in the immediately following EX executions.
% subsection hazard_detection (end)

\subsection{Forwarding} % (fold)
\label{sub:forwarding}
Forwarding works by comparing the current instructionâ€™s register operands with the destination register addresses in the preceding instructions. If a match is found, the forwarding unit manipulates the execute stage multiplexers to select the proper data to be used instead of the data provided by the decode stage. Additionally for stores preceded by loads, data can be forwarded from the write back stage back into memory if the register addresses are the same.\\ Currently the processor is able to forward memory and write back stage data back into the execute stage, as well as write back data back into memory.

% subsection forwarding (end)

% section design (end)

\section{Optimization} % (fold)
\label{sec:optimisation}
To optimize the performance of our pipelined processor, we decided to implement branch prediction hardware to reduce the number of invalid instruction fetch after the branch instructions. We first implemented a one bit predictor as a proof of concept and then implemented the two bit predictor. We compared the performance of each implementation to the non-optimized processor based on the ratio of the number of stalls to the number of branch instructions.

\subsection{One Bit Predictor} % (fold)
\label{sec:1_bit_predictor}
The first optimization we implemented was adding a 1 bit predictor to the fetch stage. The predictor monitors the output of the branch resolution hardware in the decode stage and records the result of the latest branching instruction. It also records the program counter associated with that instruction. The predictor then compares the program counter to the recorded counter for the prediction. \\
When a match occurs, it will set the next program counter address to the address it recorded when updating the prediction from the decode stage. Finally, if the prediction was wrong, the predictor will stall the wrong instruction and update the records for the next iteration.  
% section 1_bit_predictor (end)

\subsection{Two Bit Predictor} % (fold)
\label{sub:2_bit_predictor}
The next iteration of the predictor was building a two bit predictor. Its interface to the rest of the fetch and decode stage is the same, except that the internal structure uses a state machine instead of a blind comparator to drive the prediction output. The state machine was implemented according to the behavior specified in the lecture slides, lecture 13, page 17. The two bit predictor implements the same wrong prediction detection and correction strategy as the one bit predictor. 
% subsection 2_bit_predictor (end)

\subsection{Comparison} % (fold)
\label{sub:comparison}
With the three implementations completed, we created a small program implementing an infinite loop and ran it using the three implementations. First, we ran five iterations and counted the number of stalls incurred by the base implementations. Obviously, all iterations resulted in a misprediction. We did the same with the one bit predictor and the two bit predictors. In the case of the one bit predictor, only the first iteration stalled and for the two bit predictor, the first two iterations stalled.

Next we implemented a loop with an exit condition. In this case, the results were more interesting. For 10 loop iterations, the results were as follow.\\
\\
\begin{tabular}{c c c c}
& No predictor & One bit & Two Bit \\
\hline
No. of stalls & 9 & 2 & 3 \\
\hline
\\
\end{tabular}

In this case, the results are very much what we would expect. The standard processor is wrong for all except the last iteration. The one bit predictor is wrong once at the beginning of the loop, and once at the end. For the two bit predictor, it takes two iterations to get an accurate prediction and again, we observe one misprediction at the end. \\
The two bit predictor becomes interesting if we jump back to that loop again in the future. In that case, the first prediction of the second loop entry would be accurate and we would see the advantage of using the two bit predictor over the one bit predictor. 

Generally, for a single loop, the ratio of stalls over branch instructions will be as follow:
$$\eta_n = 1, \eta_1 = \frac{2}{i}, \eta_2 = \frac{3}{i}$$\\
where $\eta_n$, $\eta_1$, $\eta_2$, are the ratios applied to the basic processor, one bit predictor and two bit predictor respectively, and $i$ is the number of loop iterations.

From these equations, we can clearly see a remarkable advantage to using predictors, especially if the loops are long. However, this advantage is not as obvious with the one bit and two bit predictor. In fact, in the tests we carried out, the two bit predictor design had inferior performance when compared to the one bit predictor design. That is not to say though that it is always the case. As stated earlier, if we re-enter the same loop, then the ratio for the two bit predictor becomes $\eta_2 = \frac{1}{i}$.
% subsection comparison (end)

% section optimisation (end)

\section{Conclusion \& Discussion}
In conclusion, in this project we have successfully implemented a basic MIPS processor featuring early branch resolution, forwarding and hazard detection. \\
We then attempted to optimize that design by adding a branch prediction unit and compared the performance of different implementation for that unit.  \\
It is important to mention that the comparison results presented in this report only reflect changes in the pc address as we were unsuccessful at integrating the prediction logic past the decode stage. An example is shown in Figure~\ref{fig:pred}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.48\textwidth]{1bit-pred.png}
    \caption{Example waveform showing the effect of the predictor on the program counter. Note that the first iteration loads instruction 32 while the others do not.}
    \label{fig:pred}
\end{figure}



\begin{landscape}
\begin{@twocolumnfalse}
\section*{Figures}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=10in]{processor.png}
    \caption{5-stage MIPS pipeline implementation with buffers and forwarding lines }
    \label{fig:figure1}
\end{figure}
\end{@twocolumnfalse}
\end{landscape}


\end{document}
